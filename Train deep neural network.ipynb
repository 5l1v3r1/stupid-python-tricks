{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network model training\n",
    "\n",
    "Adapted from MNIST example from Lasagne [http://github.com/Lasagne/Lasagne].\n",
    "\n",
    "You will need to download the facial expressions dataset from the facial expressions Kaggle competition [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge]. Extract the tarball in the same directory as this notebook; ensure that the the CSV file can be found on the path ```fer2013/fer2013.csv```, where the ```fer2013``` directory is in the same path as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Choose the GPU to train on here\n",
    "GPU = 0\n",
    "theano_flags = os.environ.get('THEANO_FLAGS')\n",
    "gpu_flag = 'device=gpu{0}'.format(GPU)\n",
    "if theano_flags is None:\n",
    "    os.environ['THEANO_FLAGS'] = gpu_flag\n",
    "else:\n",
    "    os.environ['THEANO_FLAGS'] = theano_flags + ',' + gpu_flag\n",
    "\n",
    "import gzip\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "from deep_neural_network import dnn_architecture\n",
    "\n",
    "from urllib import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The location of the dataset that we use for training\n",
    "DATASET_PATH = 'fer2013/fer2013.csv'\n",
    "\n",
    "MODEL_DIR = 'dnn_model'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'dnn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going beyond 250 epochs doesn't really improve things\n",
    "NUM_EPOCHS = 250\n",
    "# Train 256 examples in parallel; reducing this may alleviate memory problems\n",
    "BATCH_SIZE = 256\n",
    "# 6 mini-batches of validation and test\n",
    "N_VALID = BATCH_SIZE * 6\n",
    "N_TEST = BATCH_SIZE * 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(seed=12345):\n",
    "    \"\"\"Get data with labels, split into training, validation and test set.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    df = pandas.read_csv(DATASET_PATH)\n",
    "    N = df.shape[0]\n",
    "\n",
    "    # Each input image is 48x48 greyscale\n",
    "    X = np.zeros((N, 1, 48, 48))\n",
    "    Y = np.zeros((N,))\n",
    "\n",
    "    for i in xrange(N):\n",
    "        # The images pixels come in the form of a string that contains uint8 (0-255) values separated by spaces\n",
    "        px = df['pixels'][i]\n",
    "        # Convert the image pixels to floats in the range 0-1\n",
    "        px = np.array([float(x)   for x in px.split()])/255.0\n",
    "        # Standardising (zero mean, unit variance) each image seems to work well\n",
    "        offset = np.mean(px)\n",
    "        scale = np.std(px)\n",
    "        if scale < 1.0e-3:\n",
    "            scale = 1.0\n",
    "        px = (px - offset) / scale\n",
    "        X[i,0,:,:] = px.reshape((48, 48))\n",
    "        Y[i] = df['emotion'][i]\n",
    "\n",
    "    indices = np.arange(X.shape[0])\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    valid_indices = indices[:N_VALID]\n",
    "    test_indices = indices[N_VALID:N_VALID+N_TEST]\n",
    "    train_indices = indices[N_VALID+N_TEST:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = Y[train_indices]\n",
    "    X_valid = X[valid_indices]\n",
    "    y_valid = Y[valid_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = Y[test_indices]\n",
    "    \n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, 48, 48))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, 48, 48))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, 48, 48))\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        y_train=y_train.astype(np.int32),\n",
    "        X_valid=lasagne.utils.floatX(X_valid),\n",
    "        y_valid=y_valid.astype(np.int32),\n",
    "        X_test=lasagne.utils.floatX(X_test),\n",
    "        y_test=y_test.astype(np.int32),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_dim=tuple([int(x) for x in X_train.shape[1:]]),\n",
    "        output_dim=7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_iter_functions(dataset, output_layer,\n",
    "                          X_tensor_type=T.tensor4,\n",
    "                          batch_size=BATCH_SIZE):\n",
    "    \"\"\"Create functions for training, validation and testing to iterate one\n",
    "       epoch.\n",
    "    \"\"\"\n",
    "    X_batch = X_tensor_type('x')\n",
    "    y_batch = T.ivector('y')\n",
    "\n",
    "    out_train = lasagne.layers.get_output(output_layer, X_batch)\n",
    "    out_eval = lasagne.layers.get_output(output_layer, X_batch, deterministic=True)\n",
    "    loss_train = lasagne.objectives.categorical_crossentropy(out_train, y_batch).mean()\n",
    "    loss_eval = lasagne.objectives.categorical_crossentropy(out_eval, y_batch).mean()\n",
    "\n",
    "    pred = T.argmax(\n",
    "        lasagne.layers.get_output(output_layer, X_batch, deterministic=True),\n",
    "        axis=1)\n",
    "    accuracy = T.mean(T.eq(pred, y_batch), dtype=theano.config.floatX)\n",
    "\n",
    "    all_params = lasagne.layers.get_all_params(output_layer)\n",
    "    updates = lasagne.updates.adadelta(\n",
    "        loss_train, all_params)\n",
    "\n",
    "    iter_train = theano.function(\n",
    "        [X_batch, y_batch], loss_train,\n",
    "        updates=updates,\n",
    "    )\n",
    "\n",
    "    iter_eval = theano.function(\n",
    "        [X_batch, y_batch], [loss_eval, accuracy],\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        train=iter_train,\n",
    "        eval=iter_eval,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(iter_funcs, dataset, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Train the model with `dataset` with mini-batch training. Each\n",
    "       mini-batch has `batch_size` recordings.\n",
    "    \"\"\"\n",
    "    num_batches_train = dataset['num_examples_train'] // batch_size\n",
    "    num_batches_valid = dataset['num_examples_valid'] // batch_size\n",
    "    num_batches_test = dataset['num_examples_test'] // batch_size\n",
    "    \n",
    "    theano_train = iter_funcs['train']\n",
    "    theano_eval = iter_funcs['eval']\n",
    "    \n",
    "    X_train = dataset['X_train']\n",
    "    y_train = dataset['y_train']\n",
    "    X_valid = dataset['X_valid']\n",
    "    y_valid = dataset['y_valid']\n",
    "    X_test = dataset['X_test']\n",
    "    y_test = dataset['y_test']\n",
    "    \n",
    "    best_epoch = 0\n",
    "    best_valid_loss = np.inf\n",
    "    best_valid_acc = 0.0\n",
    "    test_loss = np.inf\n",
    "    test_acc = 0.0\n",
    "\n",
    "    N = dataset['num_examples_train']\n",
    "    for epoch in itertools.count(1):\n",
    "        train_indices = np.arange(N).astype(np.int32)\n",
    "        np.random.shuffle(train_indices)\n",
    "        batch_train_losses = []\n",
    "        for b in range(num_batches_train):\n",
    "            batch_indices = train_indices[b*batch_size:b*batch_size+batch_size]\n",
    "            batch_train_loss = theano_train(X_train[batch_indices], y_train[batch_indices])\n",
    "            batch_train_losses.append(batch_train_loss)\n",
    "\n",
    "        avg_train_loss = np.mean(batch_train_losses)\n",
    "\n",
    "        batch_valid_losses = []\n",
    "        batch_valid_accuracies = []\n",
    "        valid_indices = np.arange(dataset['num_examples_valid']).astype(np.int32)\n",
    "        np.random.shuffle(valid_indices)\n",
    "        for b in range(num_batches_valid):\n",
    "            batch_indices = valid_indices[b*batch_size:b*batch_size+batch_size]\n",
    "            batch_valid_loss, batch_valid_accuracy = theano_eval(X_valid[batch_indices], y_valid[batch_indices])\n",
    "            batch_valid_losses.append(batch_valid_loss)\n",
    "            batch_valid_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "        avg_valid_loss = np.mean(batch_valid_losses)\n",
    "        avg_valid_accuracy = np.mean(batch_valid_accuracies)\n",
    "        \n",
    "        improved = False\n",
    "        if avg_valid_accuracy > best_valid_acc:\n",
    "            best_epoch = epoch\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            best_valid_acc = avg_valid_accuracy\n",
    "        \n",
    "            batch_test_losses = []\n",
    "            batch_test_accuracies = []\n",
    "            test_indices = np.arange(dataset['num_examples_test']).astype(np.int32)\n",
    "            np.random.shuffle(test_indices)\n",
    "            for b in range(num_batches_test):\n",
    "                batch_indices = test_indices[b*batch_size:b*batch_size+batch_size]\n",
    "                batch_test_loss, batch_test_accuracy = theano_eval(X_test[batch_indices], y_test[batch_indices])\n",
    "                batch_test_losses.append(batch_test_loss)\n",
    "                batch_test_accuracies.append(batch_test_accuracy)\n",
    "\n",
    "            test_loss = np.mean(batch_test_losses)\n",
    "            test_acc = np.mean(batch_test_accuracies)\n",
    "            improved = True\n",
    "\n",
    "        yield {\n",
    "            'number': epoch,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'valid_loss': avg_valid_loss,\n",
    "            'valid_accuracy': avg_valid_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_valid_loss': best_valid_loss,\n",
    "            'best_valid_accuracy': best_valid_acc,\n",
    "            'improved': improved,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "dataset = load_data(12345)\n",
    "\n",
    "print(\"Building model and compiling functions...\")\n",
    "output_layer, input_layer = dnn_architecture.build_model(\n",
    "    input_dim=dataset['input_dim'],\n",
    "    output_dim=dataset['output_dim'],\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "iter_funcs = create_iter_functions(dataset, output_layer)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = now = time.time()\n",
    "for epoch in train(iter_funcs, dataset):\n",
    "    print(\"Epoch {0}/{1} ({2}) in {3:.3f}s: TRAIN: loss={4:.3f}, VAL: loss={5:.3f} ({6:.3f}), acc={7:.2f}% ({8:.2f}%), TEST: loss={9:.3f}, acc={10:.2f}%\".format(\n",
    "          epoch['number'], NUM_EPOCHS, epoch['best_epoch'], time.time() - now,\n",
    "          epoch['train_loss'], epoch['valid_loss'], epoch['best_valid_loss'],\n",
    "          epoch['valid_accuracy'] * 100, epoch['best_valid_accuracy'] * 100,\n",
    "          epoch['test_loss'], epoch['test_accuracy'] * 100))\n",
    "    if epoch['improved']:\n",
    "        # Accuracy improved this epoch; save model to disk\n",
    "        all_param_values = lasagne.layers.get_all_param_values(output_layer)\n",
    "        model = {'param_values': all_param_values}\n",
    "        joblib.dump(model, MODEL_PATH)\n",
    "    now = time.time()\n",
    "\n",
    "    if epoch['number'] >= NUM_EPOCHS:\n",
    "        break\n",
    "end_time = time.time()\n",
    "\n",
    "best_valid_loss = epoch['best_valid_loss']\n",
    "best_valid_accuracy = epoch['best_valid_accuracy']\n",
    "test_loss = epoch['test_loss']\n",
    "test_accuracy = epoch['test_accuracy']\n",
    "\n",
    "print('Best epoch {0} took {1:.1f}s, valid loss={2:.3f}, valid acc={3:.2f}%, test loss={4:.3f}, test acc={5:.2f}%'.format(\n",
    "        epoch['best_epoch'], end_time-start_time,\n",
    "        best_valid_loss, best_valid_accuracy * 100,\n",
    "        test_loss, test_accuracy * 100\n",
    "    ))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
